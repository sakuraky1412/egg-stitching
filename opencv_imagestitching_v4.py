# -*- coding: utf-8 -*-
"""OpenCV-ImageStitching.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Md7HWh2ZV6_g3iCYSUw76VNr4HzxcX5

<a href="https://colab.research.google.com/github/sthalles/computer-vision/blob/master/project-4/project-4.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""
import os

import cv2
import matplotlib.pyplot as plt
import numpy
import numpy as np
import csv

#### helper methods

# helper methods for get_ROI
def get_distance_matrix(distance_matrix, k, matches):
    matches_len = len(matches)
    for i in range(0, matches_len):
        for j in range(i + 1, matches_len):
            distance_matrix[i,j] = np.math.dist(matches[i][k], matches[j][k])
    matrix_lower = np.tril_indices(matches_len, -1)
    distance_matrix[matrix_lower] = distance_matrix.T[matrix_lower]

def get_result(distance_matrix, matches):
    matches_len = len(matches)
    result_array = np.empty(matches_len, dtype=object)
    for i in range(0, matches_len):
        result_array[i] = list()
        current_index_ranking = np.argsort(distance_matrix[i])
        for j in range(0, matches_len):
            if distance_matrix[i, current_index_ranking[j]] < 50:
                result_array[i].append(current_index_ranking[j])
    return result_array

def intersection(lst1, lst2):
    # Use of hybrid method
    temp = set(lst2)
    lst3 = [value for value in lst1 if value in temp]
    return lst3

# helper methods for stitch_eggs
def get_constrained_matches(ptsA, ptsB, width):
    matches = list()
    for i, pt in enumerate(ptsA):
        if pt[1] > width * 2 / 3 and ptsB[i][1] < width * 1 / 3:
            if abs(pt[0] - ptsB[i][0]) < 50:
                matches.append((pt, ptsB[i]))
    return matches

def get_ROI(matches):
    matches_len = len(matches)
    distance_matrix_a = np.zeros((matches_len, matches_len))
    distance_matrix_b = np.zeros((matches_len, matches_len))
    get_distance_matrix(distance_matrix_a, 0, matches)
    get_distance_matrix(distance_matrix_b, 1, matches)
    result_a = get_result(distance_matrix_a, matches)
    result_b = get_result(distance_matrix_b, matches) # matches where x distance is smaller than 50
    result_dict = dict()
    for i in range(matches_len):
        current_list_a = result_a[i]
        for j in current_list_a:
            if j in result_dict:
                result_dict[j] = result_dict[j] + 1
            else:
                result_dict[j] = 1
        current_list_b = result_b[i]
        current_list_b.remove(i)
        for j in current_list_b:
            if j in result_dict:
                result_dict[j] = result_dict[j] + 1
            else:
                result_dict[j] = 1
    sorted_dict = {k: v for k, v in sorted(result_dict.items(), key=lambda item: item[1], reverse=True)[:3]}
    result_pts_a = list()
    for k in sorted_dict.keys():
        if len(result_pts_a) == 0:
            result_pts_a = result_a[k]
        else:
            result_pts_a = intersection(result_pts_a, result_a[k])
    ROI = list()
    for i, index in enumerate(result_pts_a):
        ROI.append(matches[index])

    ROI = np.array(ROI)
    return ROI

def get_displacement(ROI, default_displacement):
    if len(ROI) == 0:
        return 0, default_displacement
    ptax = [match[0][0] for match in ROI]
    ptay = [match[0][1] for match in ROI]
    ptbx = [match[1][0] for match in ROI]
    ptby = [match[1][1] for match in ROI]
    centroidA = (sum(ptax) / len(ROI), sum(ptay) / len(ROI))
    centroidB = (sum(ptbx) / len(ROI), sum(ptby) / len(ROI))
    # x = [p[0] for p in match[0] for match in ROI]
    # y = [p[1] for p in match[0] for match in ROI]
    # centroidA = (sum(x) / len(ROI), sum(y) / len(ROI))
    # x = [p[0] for p in match[1] for match in ROI]
    # y = [p[1] for p in match[1] for match in ROI]
    # centroidB = (sum(x) / len(ROI), sum(y) / len(ROI))
    displacementX = int(centroidB[0] - centroidA[0])
    displacementY = int(centroidB[1] - centroidA[1])

    if abs(displacementY) > 270:
        print(f'displacement too far {displacementY}')
        displacementY = 260

    # if abs(displacementX) > 40:
    #     print(f'displacement too far {displacementY}')
    #     if displacementX < 0:
    #         displacementX = -30
    #     else:
    #         displacementX = 30

    return -displacementX, abs(displacementY)

#### read NPM sift features

def read_single_set_of_images(egg_name):
    if "_in_" in egg_name:
        in_pos = egg_name.find("_in_")
        trainImg = cv2.imread('images/input/' + egg_name[:in_pos] + "_a" + egg_name[in_pos:] + '_EH.tif')
        queryImg = cv2.imread('images/input/' + egg_name[:in_pos] + "_b" + egg_name[in_pos:] + '_EH.tif')
        queryImg = cv2.resize(queryImg, (trainImg.shape[1], trainImg.shape[0]), interpolation=cv2.INTER_AREA)
        cImg = cv2.imread('images/input/' + egg_name[:in_pos] + "_c" + egg_name[in_pos:] + '_EH.tif')
        cImg = cv2.resize(cImg, (trainImg.shape[1], trainImg.shape[0]), interpolation=cv2.INTER_AREA)
        dImg = cv2.imread('images/input/' + egg_name[:in_pos] + "_d" + egg_name[in_pos:] + '_EH.tif')
        dImg = cv2.resize(dImg, (trainImg.shape[1], trainImg.shape[0]), interpolation=cv2.INTER_AREA)
    else:
        trainImg = cv2.imread('images/input/' + egg_name + '_a_EH.tif')
        queryImg = cv2.imread('images/input/' + egg_name + '_b_EH.tif')
        queryImg = cv2.resize(queryImg, (trainImg.shape[1], trainImg.shape[0]), interpolation=cv2.INTER_AREA)
        cImg = cv2.imread('images/input/' + egg_name + '_c_EH.tif')
        cImg = cv2.resize(cImg, (trainImg.shape[1], trainImg.shape[0]), interpolation=cv2.INTER_AREA)
        dImg = cv2.imread('images/input/' + egg_name + '_d_EH.tif')
        dImg = cv2.resize(dImg, (trainImg.shape[1], trainImg.shape[0]), interpolation=cv2.INTER_AREA)

    return trainImg, queryImg, cImg, dImg

def get_ref_and_query_egg_name(egg_name, ref_egg, query_egg):
    if "_in_" in egg_name:
        in_pos = egg_name.find("_in_")
        ref_name_tuple = (egg_name[:in_pos], ref_egg, egg_name[in_pos+1:], ref_suffix)
        query_name_tuple = (egg_name[:in_pos], query_egg, egg_name[in_pos+1:], query_suffix)
    else:
        ref_name_tuple = (egg_name, ref_egg, ref_suffix)
        query_name_tuple = (egg_name, query_egg, query_suffix)
    ref_egg_image_name = "_".join(ref_name_tuple)
    query_egg_image_name = "_".join(query_name_tuple)

    return ref_egg_image_name, query_egg_image_name

"""
Step 1: read npm sift features from csv
"""
def read_csv():
    print("Start reading npm sift features...")
    with open('matched_features_2020_200_7.csv') as csv_file:
        # suffix of each of the 4 eggs
        first_egg = "a"
        second_egg = "b"
        third_egg = "c"
        fourth_egg = "d"
        first_ptsA = []
        first_ptsB = []
        second_ptsA = []
        second_ptsB = []
        third_ptsA = []
        third_ptsB = []

        new_pair = False

        csv_reader = csv.reader(csv_file, delimiter=';')
        prev_egg_name = ''
        egg_name = ''
        for i, row in enumerate(csv_reader):
            ref_id = row[0]
            query_id = row[3]
            if ref_id == "ref_id":
                new_pair = True
                continue
            if new_pair:
                ref_len = len(ref_id)
                if prev_egg_name == '':
                    prev_egg_name = extract_egg_name(ref_id, ref_len)
                else:
                    prev_egg_name = egg_name
                egg_name = extract_egg_name(ref_id, ref_len)
                egg_names.add(egg_name)

                if len(first_ptsA) != 0:
                    image_name_to_sift_features[prev_egg_name + '_a_b'] = (first_ptsA, first_ptsB)
                    first_ptsA = []
                    first_ptsB = []
                elif len(second_ptsA) != 0:
                    image_name_to_sift_features[prev_egg_name + '_b_c'] = (second_ptsA, second_ptsB)
                    second_ptsA = []
                    second_ptsB = []
                elif len(third_ptsA) != 0:
                    image_name_to_sift_features[prev_egg_name + '_c_d'] = (third_ptsA, third_ptsB)
                    third_ptsA = []
                    third_ptsB = []

                new_pair = False
            first_ref_egg_image_name, first_query_egg_image_name = get_ref_and_query_egg_name(egg_name, first_egg,
                                                                                              second_egg)
            second_ref_egg_image_name, second_query_egg_image_name = get_ref_and_query_egg_name(egg_name, second_egg,
                                                                                                third_egg)
            third_ref_egg_image_name, third_query_egg_image_name = get_ref_and_query_egg_name(egg_name, third_egg,
                                                                                              fourth_egg)
            if ref_id == first_ref_egg_image_name and query_id == first_query_egg_image_name:
                first_ptsA.append((int(row[2]), int(row[1])))
                first_ptsB.append((int(row[5]), int(row[4])))
            if ref_id == second_ref_egg_image_name and query_id == second_query_egg_image_name:
                second_ptsA.append((int(row[2]), int(row[1])))
                second_ptsB.append((int(row[5]), int(row[4])))
            if ref_id == third_ref_egg_image_name and query_id == third_query_egg_image_name:
                third_ptsA.append((int(row[2]), int(row[1])))
                third_ptsB.append((int(row[5]), int(row[4])))

        if len(first_ptsA) != 0:
            image_name_to_sift_features[egg_name + '_a_b'] = (first_ptsA, first_ptsB)
            first_ptsA = []
            first_ptsB = []
        elif len(second_ptsA) != 0:
            image_name_to_sift_features[egg_name + '_b_c'] = (second_ptsA, second_ptsB)
            second_ptsA = []
            second_ptsB = []
        elif len(third_ptsA) != 0:
            image_name_to_sift_features[egg_name + '_c_d'] = (third_ptsA, third_ptsB)
            third_ptsA = []
            third_ptsB = []


def extract_egg_name(ref_id, ref_len):
    if "_in_" in ref_id:
        in_pos = ref_id.find("_in_")
        egg_name = ref_id[0:in_pos - 2] + ref_id[in_pos:ref_len - 7]
    else:
        egg_name = ref_id[0:ref_len - 9]
    return egg_name


"""
Step 2: read npm sift features from csv
"""
def stitch_eggs(trainImg, queryImg, cImg, dImg, egg_name):
    # the threshold where the two images will be stitched together
    height, width, channel = trainImg.shape
    thresholdA = round(width * 3 / 4)
    thresholdB = round(width * 1 / 4)
    default_displacement = thresholdA - thresholdB

    displacementX, displacementY = get_displacement_between_eggs(default_displacement, egg_name + '_a_b', width)
    displacementXBC, displacementYBC = get_displacement_between_eggs(default_displacement, egg_name + '_b_c', width)
    displacementXCD, displacementYCD = get_displacement_between_eggs(default_displacement, egg_name + '_c_d', width)

    topDisplacement = list()
    bottomDisplacement = list()
    displacements = (0, displacementX, displacementXBC, displacementXCD)
    cur_top = 0
    for displacement in displacements:
        cur_top += displacement
        cur_bottom = cur_top + height
        topDisplacement.append(cur_top)
        bottomDisplacement.append(cur_bottom)
    top = min(topDisplacement)
    bottom = max(bottomDisplacement)
    height = bottom - top
    top = - top

    ###### merging together four images, first AB ######
    oriHeight, oriWidth, oriChannel = queryImg.shape
    thresholdA = displacementY
    thresholdB = width - thresholdA
    cur_top = top + displacementX
    movedQuery = numpy.zeros((height, thresholdA + width, 3), dtype=np.uint8)
    movedQuery[cur_top: cur_top + oriHeight, thresholdA:] = queryImg[:, :]
    # for debug use
    # print(movedQuery.shape)
    # plt.figure(figsize=(10, 5))
    # plt.imshow(movedQuery)
    # plt.show()
    result = numpy.zeros((height, thresholdA + width, 3), dtype=np.uint8)
    result[top:top + trainImg.shape[0], :trainImg.shape[1]] = trainImg
    # save trainImg for use in alpha blending
    resultCopy = result.copy()
    # print(movedQuery.shape)
    # plt.figure(figsize=(10, 5))
    # plt.imshow(resultCopy)
    # plt.show()
    leftBorder = numpy.zeros((height, thresholdA + width), dtype=bool)
    rightBorder = numpy.zeros((height, thresholdA + width), dtype=bool)
    for i in range(0, height):
        for j in range(thresholdA, width):
            if (movedQuery[i, j] > [0, 0, 0]).all():
                leftBorder[i][j] = True
    for i in range(0, height):
        for j in range(thresholdA, width):
            if (result[i, j] > [0, 0, 0]).all():
                rightBorder[i][j] = True
    # print('here')
    for i in range(0, height):
        for j in range(thresholdA, thresholdA + width):
            if (movedQuery[i, j] > [0, 0, 0]).all():
                if (result[i, j] == [0, 0, 0]).all():
                    result[i, j] = movedQuery[i, j]
                else:
                    if not (leftBorder[i, j] and j < width - (thresholdB / 2)) and rightBorder[i, j]:
                        result[i, j] = movedQuery[i, j]
    window = 50
    suture = round(width - (thresholdB / 2))
    leftWidth = suture - round(window / 2)
    rightWidth = suture + round(window / 2)
    for i in range(0, height):
        for j in range(leftWidth, rightWidth):
            if (resultCopy[i, j] > [0, 0, 0]).all() and (movedQuery[i, j] > [0, 0, 0]).all():
                a = (rightWidth - j) / window
                result[i, j] = resultCopy[i, j] * a + movedQuery[i, j] * (1 - a)
    # plt.figure(figsize=(10, 5))
    # plt.imshow(result)
    # plt.show()
    ###### merge BC ######
    thresholdA = displacementYBC
    thresholdB = oriWidth - thresholdA
    trainImg = result
    queryImg = cImg
    height, width, channel = trainImg.shape
    cur_top += displacementXBC
    movedQuery = numpy.zeros((height, thresholdA + width, 3), dtype=np.uint8)
    movedQuery[cur_top:cur_top + oriHeight, width + thresholdA - oriWidth:] = queryImg[:, :]
    # for debug use
    # print(movedQuery.shape)
    # plt.figure(figsize=(10, 5))
    # plt.imshow(movedQuery)
    # plt.show()
    result = numpy.zeros((height, thresholdA + width, 3), dtype=np.uint8)
    result[:trainImg.shape[0], :trainImg.shape[1]] = trainImg
    # save trainImg for use in alpha blending
    resultCopy = result.copy()
    leftBorder = numpy.zeros((height, thresholdA + width), dtype=bool)
    rightBorder = numpy.zeros((height, thresholdA + width), dtype=bool)
    for i in range(0, height):
        for j in range(thresholdA, width):
            if (movedQuery[i, j] > [0, 0, 0]).all():
                leftBorder[i][j] = True
    for i in range(0, height):
        for j in range(thresholdA, width):
            if (result[i, j] > [0, 0, 0]).all():
                rightBorder[i][j] = True
    # print('here')
    for i in range(0, height):
        for j in range(thresholdA, thresholdA + width):
            if (movedQuery[i, j] > [0, 0, 0]).all():
                if (result[i, j] == [0, 0, 0]).all():
                    result[i, j] = movedQuery[i, j]
                else:
                    if not (leftBorder[i, j] and j < width - (thresholdB / 2)) and rightBorder[i, j]:
                        result[i, j] = movedQuery[i, j]
    window = 50
    suture = round(width - (thresholdB / 2))
    leftWidth = suture - round(window / 2)
    rightWidth = suture + round(window / 2)
    for i in range(0, height):
        for j in range(leftWidth, rightWidth):
            if (resultCopy[i, j] > [0, 0, 0]).all() and (movedQuery[i, j] > [0, 0, 0]).all():
                a = (rightWidth - j) / window
                result[i, j] = resultCopy[i, j] * a + movedQuery[i, j] * (1 - a)
    # plt.figure(figsize=(10, 5))
    # plt.imshow(result)
    # plt.show()
    ###### merge CD ######
    thresholdA = displacementYCD
    thresholdB = oriWidth - thresholdA
    trainImg = result
    queryImg = dImg
    height, width, channel = trainImg.shape
    cur_top += displacementXCD
    movedQuery = numpy.zeros((height, thresholdA + width, 3), dtype=np.uint8)
    movedQuery[cur_top:cur_top + oriHeight, thresholdA + width - oriWidth:] = queryImg[:, :]
    # for debug use
    # print(movedQuery.shape)
    # plt.figure(figsize=(10, 5))
    # plt.imshow(movedQuery)
    # plt.show()
    result = numpy.zeros((height, thresholdA + width, 3), dtype=np.uint8)
    result[:trainImg.shape[0], :trainImg.shape[1]] = trainImg
    # save trainImg for use in alpha blending
    resultCopy = result.copy()
    leftBorder = numpy.zeros((height, thresholdA + width), dtype=bool)
    rightBorder = numpy.zeros((height, thresholdA + width), dtype=bool)
    for i in range(0, height):
        for j in range(thresholdA, width):
            if (movedQuery[i, j] > [0, 0, 0]).all():
                leftBorder[i][j] = True
    for i in range(0, height):
        for j in range(thresholdA, width):
            if (result[i, j] > [0, 0, 0]).all():
                rightBorder[i][j] = True
    # print('here')
    for i in range(0, height):
        for j in range(thresholdA, thresholdA + width):
            if (movedQuery[i, j] > [0, 0, 0]).all():
                if (result[i, j] == [0, 0, 0]).all():
                    result[i, j] = movedQuery[i, j]
                else:
                    if not (leftBorder[i, j] and j < width - (thresholdB / 2)) and rightBorder[i, j]:
                        result[i, j] = movedQuery[i, j]
    window = 50
    suture = round(width - (thresholdB / 2))
    leftWidth = suture - round(window / 2)
    rightWidth = suture + round(window / 2)
    for i in range(0, height):
        for j in range(leftWidth, rightWidth):
            if (resultCopy[i, j] > [0, 0, 0]).all() and (movedQuery[i, j] > [0, 0, 0]).all():
                a = (rightWidth - j) / window
                result[i, j] = resultCopy[i, j] * a + movedQuery[i, j] * (1 - a)
    # plt.figure(figsize=(10, 5))
    # plt.imshow(result)
    # plt.show()
    cv2.imwrite(f"images/output/{egg_name}_stitched.tif", result)


def get_displacement_between_eggs(default_displacement, egg_name_key, width):
    if egg_name_key in image_name_to_sift_features:
        ptsA = image_name_to_sift_features[egg_name_key][0]
        ptsB = image_name_to_sift_features[egg_name_key][1]
        # restrict matches to the sides of the eggs and points are close to each other
        matches = get_constrained_matches(ptsA, ptsB, width)
        # ROI = [matches[1], matches[2], matches[5]]
        ROI = get_ROI(matches)
        # Get x and y displacements of the matched points for the two images
        displacementX, displacementY = get_displacement(ROI, default_displacement)
    else:
        displacementX = 0
        displacementY = default_displacement
    return displacementX, displacementY


ref_suffix = "CR.tif"
query_suffix = "EH.TIF"

image_name_to_sift_features = dict()
egg_names = set()

read_csv()

for i, egg_name in enumerate(egg_names):
    print(f"Egg {i+1}/{len(egg_names)}")
# egg_name = "2019_PS094_P3" 195 291
# egg_name = "2019_PSK08_P2" 195 308
# egg_name = "2019_PSK01pre_A2" 234 211 330
    print(f"Start reading {egg_name}...")
    trainImg, queryImg, cImg, dImg = read_single_set_of_images(egg_name)
    print(f"Start stitching {egg_name}...")
    stitch_eggs(trainImg, queryImg, cImg, dImg, egg_name)

# /Users/christine/Downloads/Tanmay_work/Egg_tifs_masks/NPM_working/*/*_EH.tif
#
# /Users/christine/PycharmProjects/image-stitching-opencv/images/input/
# archive code
###### START ######

# method options
# feature_extractor = 'orb'  # one of 'sift', 'surf', 'brisk', 'orb'
# feature_matching = 'knn'  # one of 'bf', 'knn'

# def detectAndDescribe(image, method=None):
#     """
#         Compute key points and feature descriptors using an specific method
#     """
#     assert method is not None, "You need to define a feature detection method. Values are: 'sift', 'surf'"
#
#     # detect and extract features from the image
#     if method == 'sift':
#         descriptor = cv2.xfeatures2d.SIFT_create()
#     elif method == 'surf':
#         descriptor = cv2.xfeatures2d.SURF_create()
#     elif method == 'brisk':
#         descriptor = cv2.BRISK_create()
#     elif method == 'orb':
#         descriptor = cv2.ORB_create()
#
#     # get keypoints and descriptors
#     (kps, features) = descriptor.detectAndCompute(image, None)
#
#     return (kps, features)

# def createMatcher(method, crossCheck):
#     """
#     Create and return a Matcher Object
#     """
#     if method == 'sift' or method == 'surf':
#         bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=crossCheck)
#     elif method == 'orb' or method == 'brisk':
#         bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=crossCheck)
#     return bf
#
#
# def matchKeyPointsBF(featuresA, featuresB, method):
#     bf = createMatcher(method, crossCheck=True)
#
#     # Match descriptors.
#     best_matches = bf.match(featuresA, featuresB)
#
#     # Sort the features in order of distance.
#     # The points with small distance (more similarity) are ordered first in the vector
#     rawMatches = sorted(best_matches, key=lambda x: x.distance)
#     print("Raw matches (Brute force):", len(rawMatches))
#     return rawMatches
#
#
# def matchKeyPointsKNN(featuresA, featuresB, ratio, method):
#     bf = createMatcher(method, crossCheck=False)
#     # compute the raw matches and initialize the list of actual matches
#     rawMatches = bf.knnMatch(featuresA, featuresB, 2)
#     print("Raw matches (knn):", len(rawMatches))
#     matches = []
#
#     # loop over the raw matches
#     for m, n in rawMatches:
#         # ensure the distance is within a certain ratio of each
#         # other (i.e. Lowe's ratio test)
#         # if m.distance < n.distance * ratio:
#         matches.append(m)
#     return matches

# def get_matches(trainImg, queryImg, trainImg_gray, queryImg_gray):
#     kpsA, featuresA = detectAndDescribe(trainImg_gray, method=feature_extractor)
#     kpsB, featuresB = detectAndDescribe(queryImg_gray, method=feature_extractor)
#     print("Using: {} feature matcher".format(feature_matching))
#     if feature_matching == 'bf':
#         matches = matchKeyPointsBF(featuresA, featuresB, method=feature_extractor)
#         img3 = cv2.drawMatches(trainImg, kpsA, queryImg, kpsB, matches[:100],
#                                None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
#     elif feature_matching == 'knn':
#         matches = matchKeyPointsKNN(featuresA, featuresB, ratio=0.9, method=feature_extractor)
#         img3 = cv2.drawMatches(trainImg, kpsA, queryImg, kpsB, np.random.choice(matches, 100),
#                                None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
#     return kpsA, kpsB, matches

# def getHomography(kpsA, kpsB, matches, reprojThresh):
#     # convert the keypoints to numpy arrays
#     kpsA = np.float32([kp.pt for kp in kpsA])
#     kpsB = np.float32([kp.pt for kp in kpsB])
#
#     if len(matches) > 4:
#
#         # construct the two sets of points
#         ptsA = np.float32([kpsA[m.queryIdx] for m in matches])
#         ptsB = np.float32([kpsB[m.trainIdx] for m in matches])
#
#         # estimate the homography between the sets of points
#         (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,
#                                          reprojThresh)
#         # return (matches, H, status)
#         # instead of the homography return the locations of the matched features
#         return ptsA, ptsB
#     else:
#         return None

# def alpha_blending(im1, im2, window_size=0.5):
#     """
#     return a new image that smoothly combines im1 and im2
#     im1: np.array image of the dimensions: height x width x channels; values: 0-1
#     im2: np.array same dim as im1
#     window_size: what fraction of image width to use for the transition (0-1)
#     """
#     # useful functions: np.linspace and np.concatenate
#     assert (im1.shape == im2.shape)
#     h, w, c = im1.shape
#     new_im = im1.copy()
#     # calculate the actual window size and the position where the window starts
#     window = int(window_size * w)
#     win_start = int((1 - window_size) * w / 2)
#     # replace half of the image as in hard blending
#     new_im[:, :win_start, :] = im2[:, :win_start, :]
#     # use alpha mask to blend the images
#     for i in range(0, window):
#         a = i / window
#         new_im[:, win_start + i, :] = im1[:, win_start + i, :] * a + im2[:, win_start + i, :] * (1 - a)
#
#     return new_im

# read images and transform them to grayscale
# make sure that the train image is the image that will be transformed

# code for reading images in a directory
# images = []
# image_dir = 'images/input/'
# image_names = os.listdir(image_dir)
# image_names.remove('.DS_Store')
# image_names.sort()
# image_names_remove_duplicate = set()
#
# for img_id, image_name in enumerate(image_names):
#     image_path = os.path.join(image_dir, image_name)
#     image = cv2.imread(image_path)
#     images.append(image)
#
# for image_name in image_names:
#     egg_name = image_name[0:len(image_name) - 9]
#     image_names_remove_duplicate.add(egg_name)

# opencv defines the color channel in the order BGR.
# transform it to RGB to be compatible to matplotlib
# TODO: fix bug where edge of egg is missing
# TODO: fix bug where two eggs are too far apart
# TODO: clean up
# solution：arbitrary merge if no good distance

# def get_ref_egg_name(egg_name, ref_egg):
#     ref_name_tuple = (egg_name, ref_egg, ref_suffix)
#     ref_egg_image_name = "_".join(ref_name_tuple)
#
#     return ref_egg_image_name

# def get_egg_name(full_egg_name):
#     end_index = full_egg_name.index("_a_CR.tif")
#     egg_name = full_egg_name[:end_index]
#
#     return egg_name

# old method of getting matched sift features
# kpsA, kpsB, matches = get_matches(trainImg_gray, queryImg_gray)
# kpsB_, kpsC, matchesBC = get_matches(queryImg_gray, cImg_gray)
# kpsC_, kpsD, matchesCD = get_matches(cImg_gray, dImg_gray)
#
# ###### Modified code below ######
#
# M = getHomography(kpsA, kpsB, matches, reprojThresh=4)
# if M is None:
#     print("Error: M is None")
# MBC = getHomography(kpsB_, kpsC, matchesBC, reprojThresh=4)
# if MBC is None:
#     print("Error: MBC is None")
# MCD = getHomography(kpsC_, kpsD, matchesCD, reprojThresh=4)
# if MCD is None:
#     print("Error: MCD is None")
#
# # matched points in the two images
# ptsA, ptsB = M
# ptsB_, ptsC = MBC
# ptsC_, ptsD = MCD

###### END ######

# # Apply panorama correction
# width = trainImg.shape[1] + queryImg.shape[1]
# height = trainImg.shape[0] + queryImg.shape[0]
#
# result = cv2.warpPerspective(trainImg, H, (width, height))
# result[0:queryImg.shape[0], 0:queryImg.shape[1]] = queryImg
#
# plt.figure(figsize=(20,10))
# plt.imshow(result)
#
# plt.axis('off')
# plt.show()

# # transform the panorama image to grayscale and threshold it
# gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)
# thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)[1]
#
# # Finds contours from the binary image
# cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
# cnts = imutils.grab_contours(cnts)
#
# # get the maximum contour area
# c = max(cnts, key=cv2.contourArea)
#
# # get a bbox from the contour area
# (x, y, w, h) = cv2.boundingRect(c)
#
# # crop the image to the bbox coordinates
# result = result[y:y + h, x:x + w]
#
# # show the cropped image
# plt.figure(figsize=(20,10))
# plt.imshow(result)
#
# plt.axis('off')
# plt.show()


# store old code
# height = height + displacementX
# width = width + displacementY
#
# movedQuery = numpy.zeros((height, thresholdA + width, 3), dtype=np.uint8)
# movedQuery[displacementX:, thresholdA + displacementY:] = queryImg[:, :]
#
# # for debug use
# # print(movedQuery.shape)
# # plt.figure(figsize=(10, 5))
# # plt.imshow(movedQuery)
# # plt.show()
#
# result = numpy.zeros((height, thresholdA + width, 3), dtype=np.uint8)
# result[:trainImg.shape[0], :trainImg.shape[1]] = trainImg
# # save trainImg for use in alpha blending
# resultCopy = result
#
# leftBorder = numpy.zeros((height, thresholdA + width), dtype=bool)
# rightBorder = numpy.zeros((height, thresholdA + width), dtype=bool)
#
# for i in range(0 + displacementX, height):
#     for j in range(thresholdA + displacementY, width):
#         if (movedQuery[i, j] > [10, 10, 10]).all():
#             leftBorder[i][j] = True
#
# for i in range(0, height - displacementX):
#     for j in range(thresholdA, width - displacementY):
#         if (result[i, j] > [10, 10, 10]).all():
#             rightBorder[i][j] = True
#
# # print('here')
#
# for i in range(0, height):
#     for j in range(thresholdA + displacementY, thresholdA + width):
#         if (movedQuery[i, j] > [10, 10, 10]).all():
#             if (result[i, j] < [10, 10, 10]).all():
#                 result[i, j] = movedQuery[i, j]
#             else:
#                 if leftBorder[i, j] and j < width - (thresholdB * 0.5):
#                     result[i, j] = result[i, j]
#                 elif rightBorder[i, j]:
#                     result[i, j] = movedQuery[i, j]
#         else:
#             result[i, j] = result[i, j]
#
# window = 40
# imageWidth = round((thresholdA + width) / 2)
# leftWidth = imageWidth - round(window/2)
# rightWidth = imageWidth + round(window/2)
# for i in range(0, height):
#     for j in range(leftWidth, rightWidth):
#         if (resultCopy[i, j] > [10, 10, 10]).all() and (movedQuery[i, j] > [10, 10, 10]).all():
#             a = (rightWidth - j) / window
#             result[i, j] = trainImg[i, j] * a + movedQuery[i, j] * (1-a)