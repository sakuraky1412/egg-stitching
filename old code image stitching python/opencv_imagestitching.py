# -*- coding: utf-8 -*-
"""OpenCV-ImageStitching.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Md7HWh2ZV6_g3iCYSUw76VNr4HzxcX5

<a href="https://colab.research.google.com/github/sthalles/computer-vision/blob/master/project-4/project-4.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

import cv2
import numpy
import numpy as np
import matplotlib.pyplot as plt
import imageio
import imutils

cv2.ocl.setUseOpenCL(False)

# select the image id (valid values 1,2,3, or 4)
feature_extractor = 'orb'  # one of 'sift', 'surf', 'brisk', 'orb'
feature_matching = 'knn'

"""## ReadME

- When choosing the images bellow, make sure that the **train image** is the image to be transformed. 

- For the images in the input folder, the image with id 3 should be inverted so that **train image** is the image B and **query image** is the image A.
"""

# read images and transform them to grayscale
# Make sure that the train image is the image that will be transformed
trainImg = cv2.imread('images/output/2019_PS089_P1_a.tif')
trainImg_gray = cv2.cvtColor(trainImg, cv2.COLOR_BGR2GRAY)

queryImg = cv2.imread('images/output/2019_PS089_P1_b.tif')
queryImg = cv2.resize(queryImg, (trainImg.shape[1], trainImg.shape[0]), interpolation=cv2.INTER_AREA)

# Opencv defines the color channel in the order BGR. 
# Transform it to RGB to be compatible to matplotlib
queryImg_gray = cv2.cvtColor(queryImg, cv2.COLOR_BGR2GRAY)


# fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, constrained_layout=False, figsize=(16,9))
# ax1.imshow(queryImg, cmap="gray")
# ax1.set_xlabel("Query image", fontsize=14)
#
# ax2.imshow(trainImg, cmap="gray")
# ax2.set_xlabel("Train image (Image to be transformed)", fontsize=14)
#
# plt.show()

def detectAndDescribe(image, method=None):
    """
    Compute key points and feature descriptors using an specific method
    """

    assert method is not None, "You need to define a feature detection method. Values are: 'sift', 'surf'"

    # detect and extract features from the image
    if method == 'sift':
        descriptor = cv2.xfeatures2d.SIFT_create()
    elif method == 'surf':
        descriptor = cv2.xfeatures2d.SURF_create()
    elif method == 'brisk':
        descriptor = cv2.BRISK_create()
    elif method == 'orb':
        descriptor = cv2.ORB_create()

    # get keypoints and descriptors
    (kps, features) = descriptor.detectAndCompute(image, None)

    return (kps, features)


kpsA, featuresA = detectAndDescribe(trainImg_gray, method=feature_extractor)
kpsB, featuresB = detectAndDescribe(queryImg_gray, method=feature_extractor)


# # display the keypoints and features detected on both images
# fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20,8), constrained_layout=False)
# ax1.imshow(cv2.drawKeypoints(trainImg_gray,kpsA,None,color=(0,255,0)))
# ax1.set_xlabel("(a)", fontsize=14)
# ax2.imshow(cv2.drawKeypoints(queryImg_gray,kpsB,None,color=(0,255,0)))
# ax2.set_xlabel("(b)", fontsize=14)
#
# plt.show()

def createMatcher(method, crossCheck):
    "Create and return a Matcher Object"

    if method == 'sift' or method == 'surf':
        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=crossCheck)
    elif method == 'orb' or method == 'brisk':
        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=crossCheck)
    return bf


def matchKeyPointsBF(featuresA, featuresB, method):
    bf = createMatcher(method, crossCheck=True)

    # Match descriptors.
    best_matches = bf.match(featuresA, featuresB)

    # Sort the features in order of distance.
    # The points with small distance (more similarity) are ordered first in the vector
    rawMatches = sorted(best_matches, key=lambda x: x.distance)
    print("Raw matches (Brute force):", len(rawMatches))
    return rawMatches


def matchKeyPointsKNN(featuresA, featuresB, ratio, method):
    bf = createMatcher(method, crossCheck=False)
    # compute the raw matches and initialize the list of actual matches
    rawMatches = bf.knnMatch(featuresA, featuresB, 2)
    print("Raw matches (knn):", len(rawMatches))
    matches = []

    # loop over the raw matches
    for m, n in rawMatches:
        # ensure the distance is within a certain ratio of each
        # other (i.e. Lowe's ratio test)
        # if m.distance < n.distance * ratio:
        matches.append(m)
    return matches


print("Using: {} feature matcher".format(feature_matching))

# fig = plt.figure(figsize=(20,8))

if feature_matching == 'bf':
    matches = matchKeyPointsBF(featuresA, featuresB, method=feature_extractor)
    img3 = cv2.drawMatches(trainImg, kpsA, queryImg, kpsB, matches[:100],
                           None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
elif feature_matching == 'knn':
    matches = matchKeyPointsKNN(featuresA, featuresB, ratio=0.9, method=feature_extractor)
    img3 = cv2.drawMatches(trainImg, kpsA, queryImg, kpsB, np.random.choice(matches, 100),
                           None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)


# plt.imshow(img3)
# plt.show()

def getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh):
    # convert the keypoints to numpy arrays
    kpsA = np.float32([kp.pt for kp in kpsA])
    kpsB = np.float32([kp.pt for kp in kpsB])

    if len(matches) > 4:

        # construct the two sets of points
        ptsA = np.float32([kpsA[m.queryIdx] for m in matches])
        ptsB = np.float32([kpsB[m.trainIdx] for m in matches])

        # estimate the homography between the sets of points
        (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,
                                         reprojThresh)
        # return (matches, H, status)
        return ptsA, ptsB
    else:
        return None


# M = getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh=4)
# if M is None:
#     print("Error!")
# (matches, H, status) = M
# print(H)

M = getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh=4)
if M is None:
    print("Error!")

ptsA, ptsB = M

matchesA = []
matchesB = []

height, width, channel = trainImg.shape
thresholdA = round(width * 2 / 3)
thresholdB = round(width * 1 / 3)

matches = []

for i, pt in enumerate(ptsA):
    if pt[1] > width * 2 / 3 and ptsB[i][1] < width * 1 / 3:
        if abs(pt[0] - ptsB[i][0]) < 100:
            matches.append((pt, ptsB[i]))

# Apply panorama correction
# width = trainImg.shape[1] + queryImg.shape[1]
# height = trainImg.shape[0] + queryImg.shape[0]
# 125
trainX, trainY = matches[2][0]
queryX, queryY = matches[2][1]

ROI = [matches[1], matches[2], matches[5]]
x = [p[0] for p in ROI[0]]
y = [p[1] for p in ROI[0]]
centroidA = (sum(x) / len(ROI), sum(y) / len(ROI))

x = [p[0] for p in ROI[1]]
y = [p[1] for p in ROI[1]]
centroidB = (sum(x) / len(ROI), sum(y) / len(ROI))

displacement = (centroidB[0] - centroidA[0], centroidB[1] - centroidA[1])


print(trainImg.shape)
# print(queryImg.shape)
# print(thresholdA)
# print(thresholdB)
# print(width-thresholdB)
# print(height)
# print(trainX)
# print(trainY)
# print(queryX)
# print(queryY)

# result = cv2.warpPerspective(trainImg, H, (width, height))
result = cv2.hconcat([trainImg, queryImg])
result = result[:, :thresholdA + width]
result[:, 0:trainImg.shape[1]] = trainImg
result[:, trainImg.shape[1]:] = [0, 0, 0]

# result[:,thresholdA:thresholdA+thresholdA] = queryImg[:,width-thresholdA:width]

def alpha_blending(im1, im2, window_size=0.5):
    """
  return a new image that smoothly combines im1 and im2
  im1: np.array image of the dimensions: height x width x channels; values: 0-1
  im2: np.array same dim as im1
  window_size: what fraction of image width to use for the transition (0-1)
  """
    # useful functions: np.linspace and np.concatenate
    assert (im1.shape == im2.shape)
    h, w, c = im1.shape
    new_im = im1.copy()
    # calculate the actual window size and the position where the window starts
    window = int(window_size * w)
    win_start = int((1 - window_size) * w / 2)
    # replace half of the image as in hard blending
    new_im[:, :win_start, :] = im2[:, :win_start, :]
    # use alpha mask to blend the images
    for i in range(0, window):
        a = i / window
        new_im[:, win_start + i, :] = im1[:, win_start + i, :] * a + im2[:, win_start + i, :] * (1 - a)

    return new_im


leftBorder = numpy.zeros((height, thresholdA + width), dtype=bool)
rightBorder = numpy.zeros((height, thresholdA + width), dtype=bool)

for i in range(0, height):
    for j in range(thresholdA, width):
        if (queryImg[i, j - thresholdA] < [10, 10, 10]).all():
            leftBorder[i][j] = True
        else:
            leftBorder[i][j+50] = True
# print('here')

for i in range(0, height):
    for j in range(thresholdA, thresholdA + width):
        if (result[i, j] > [10, 10, 10]).all():
            rightBorder[i][j] = True
        else:
            rightBorder[i][j+50] = True
            break
print('here')

window = int(width * 0.5)
# print(window)
for i in range(0, height):
    for j in range(thresholdA, thresholdA + width):
        # if (queryImg[i, j - thresholdA] != [0, 0, 0]).all():
        if (queryImg[i, j - thresholdA] > [10, 10, 10]).all():
            if (result[i, j] < [10, 10, 10]).all():
                result[i, j] = queryImg[i, j - thresholdA]
            else:
                a = (j - thresholdA) / (j - 50)
                print(a)
                if not leftBorder[i,j] and j < width - (thresholdB * 0.5):
                # if not leftBorder[i, j]:
                    result[i, j] = result[i, j]
                    # result[i, j] = result[i, j] * (1 - a) + queryImg[i, j - thresholdA] * a
                # elif not rightBorder[i,j]:
                #     result[i, j] = result[i, j] * 0.1 + queryImg[i, j - thresholdA] * 0.9
                # else:
                #     a = (j - thresholdA) / window
                #     print(a)
                #     result[i, j] = result[i, j] * a + queryImg[i, j - thresholdA] * (1 - a)
                # elif not rightBorder[i,j]:
                #     a = (j - thresholdA) / (j - 50)
                #     result[i, j] = result[i, j] * a + queryImg[i, j - thresholdA] * (1 - a)
                # elif j == width - (thresholdB * 0.5):
                #     if (result[i, j] > [10, 10, 10]).all() and (queryImg[i, j - thresholdA] > [10, 10, 10]).all():
                #         result[i, j] = result[i, j] * 0.5 + queryImg[i, j - thresholdA] * 0.5
                elif rightBorder[i,j]:
                    result[i, j] = queryImg[i, j - thresholdA]
                # else:
                #     result[i, j] = result[i, j] * a + queryImg[i, j - thresholdA] * (1-a)
                    # result[i, j] = queryImg[i, j - thresholdA]
        else:
            result[i, j] = result[i, j]

window = 40
imageWidth = round((thresholdA + width) / 2)
for i in range(0, height):
    for j in range(imageWidth - 20, imageWidth + 20):
        if (trainImg[i, j] > [10, 10, 10]).all() and (queryImg[i, j - thresholdA] > [10, 10, 10]).all():
            a = (imageWidth + 20 - j) / window
            # print(a)
            result[i, j] = trainImg[i, j] * a + queryImg[i, j - thresholdA] * (1 - a)

plt.figure(figsize=(20, 10))
plt.imshow(result)

plt.axis('off')
plt.show()

# # Apply panorama correction
# width = trainImg.shape[1] + queryImg.shape[1]
# height = trainImg.shape[0] + queryImg.shape[0]
#
# result = cv2.warpPerspective(trainImg, H, (width, height))
# result[0:queryImg.shape[0], 0:queryImg.shape[1]] = queryImg
#
# plt.figure(figsize=(20,10))
# plt.imshow(result)
#
# plt.axis('off')
# plt.show()

#
# # transform the panorama image to grayscale and threshold it
# gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)
# thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)[1]
#
# # Finds contours from the binary image
# cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
# cnts = imutils.grab_contours(cnts)
#
# # get the maximum contour area
# c = max(cnts, key=cv2.contourArea)
#
# # get a bbox from the contour area
# (x, y, w, h) = cv2.boundingRect(c)
#
# # crop the image to the bbox coordinates
# result = result[y:y + h, x:x + w]
#
# # show the cropped image
# plt.figure(figsize=(20,10))
# plt.imshow(result)
#
# plt.axis('off')
# plt.show()
